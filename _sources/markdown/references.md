# References

<a name="chang2022styleeq"></a>Chang, J.-H. R., Shrivastava, A., Koppula, H. S., Zhang, X., & Tuzel, O. (2022). Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models (2022). PMLR 2022.

<a name="fang2019pretrained"></a>Fang, W., Chung, Y.-A., & Glass, J. (2019). Towards Transfer Learning for End-to-End Speech Synthesis from Deep Pre-Trained Language Models (arXiv:1906.07307). [arXiv:1906.07307](https://arxiv.org/abs/1906.07307).

<a name="fogel2020scrabblegan"></a>Fogel, S., Averbuch-Elor, H., Cohen, S., Mazor, S., & Litman, R. (2020). ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation. CVPR 2020.

<a name="gales2009svm"></a>Gales, M. J. F., Ragni, A., AlDamarki, H., & Gautier, C. (2009). Support vector machines for noise robust ASR. ASRU 2009.

<a name="grffinlim1984"></a>Griffin, D. & Jae Lim. (1984). Signal estimation from modified short-time Fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing 1984.

<a name="hsu2018vae"></a>Hsu, W.-N., Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Wang, Y., Cao, Y., Jia, Y., Chen, Z., Shen, J., Nguyen, P., & Pang, R. (2018). Hierarchical Generative Modeling for Controllable Speech Synthesis, ICML 2018.

<a name="hsu2019noise"></a>Hsu, W.-N., Zhang, Y., Weiss, R. J., Chung, Y.-A., Wang, Y., Wu, Y., & Glass, J. (2019). Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization. ICASSP 2019.

<a name="hu2022synt"></a>Hu, T.-Y., Armandpour, M., Shrivastava, A., Chang, J.-H. R., Koppula, H., & Tuzel, O. (2022). SYNT++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition. ICASSP 2022.

<a name="jaitly2013vtlp"></a>Jaitly, N., & Hinton, G. E. (n.d.). Vocal Tract Length Perturbation (VTLP) improves speech recognition. ICML 2013.

<a name="kim2021vits"></a>Kim, J., Kong, J., & Son, J. (n.d.). Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech. PMLR 2021

<a name="kingmawelling2014vae"></a>Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes [arXiv:1312.6114](http://arxiv.org/abs/1312.6114).


<a name="kuznichov2019leaf"></a>Kuznichov, D., Zvirin, A., Honen, Y., & Kimmel, R. (2019). Data Augmentation for Leaf Segmentation and Counting Tasks in Rosette Plants. CVF 2019.

<a name="li2018ttsasr"></a>Li, J., Gadde, R., Ginsburg, B., & Lavrukhin, V. (2018). Training Neural Speech Recognition Systems with Synthetic Speech Augmentation. [arXiv:1811.00707](http://arxiv.org/abs/1811.00707).

<a name="li2019transformertts"></a>Li, N., Liu, S., Liu, Y., Zhao, S., Liu, M., & Zhou, M. (2019). Neural Speech Synthesis with Transformer Network (arXiv:1809.08895). AAAI 2019.

<a name="nikolenko2021synthetic"></a>Nikolenko, S. I. (2021). Synthetic Data for Deep Learning (Vol. 174). Springer International Publishing.

<a name="oord2016wavenet"></a>Oord, A. van den, Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., & Kavukcuoglu, K. (2016). WaveNet: A Generative Model for Raw Audio. [arXiv:1609.03499](https://arxiv.org/abs/1609.03499).

<a name="oord2017vqvae"></a>Oord, A. van den, Vinyals, O., & Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. NeurIPS 2017.

<a name="park2019specaugment"></a>Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., & Le, Q. V. (2019). SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. Interspeech 2019.

<a name="pine2022lowresourcefastspeech"></a>Pine, A., Wells, D., Brinklow, N., Littell, P., & Richmond, K. (2022). Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization. ACL 2022.

<a name="povey2006fmllr"></a>Povey, D., & Saon, G. (2006). Feature and model space speaker adaptation with full covariance Gaussians. Interspeech 2006.

<a name="ren2021fastspeech2"></a>Ren, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., & Liu, T.-Y. (2021). FastSpeech 2: Fast and High-Quality End-to-End Text to Speech. ICLR 2021.

<a name="rossenbach2020ttsasr"></a>Rossenbach, N., Zeyer, A., Schlüter, R., & Ney, H. (2020). Generating Synthetic Audio Data for Attention-Based Speech Recognition Systems (arXiv:1912.09257). ICASSP 2020.

<a name="rygaard2015lowresource"></a>Rygaard, L. V. (2015). Using synthesized speech to improve speech recognition for lowresource languages.

<a name="shen2018tacotron2"></a>Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerry-Ryan, R. J., Saurous, R. A., Agiomyrgiannakis, Y., & Wu, Y. (2018). Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ICASSP 2018.

<a name="stone2020articulatory"></a>Stone, S., Azgin, A., Mänz, S., & Birkholz, P. (2020). Prospects of articulatory text-to-speech synthesis. ISSP 2020.

<a name="tan2021survey"></a>Tan, X., Qin, T., Soong, F., & Liu, T.-Y. (2021). A Survey on Neural Speech Synthesis. [arXiv:2106.15561](https://arxiv.org/abs/2106.15561).

<a name="thai2019improvinglowresource"></a>Thai, B., Jimerson, R., Arcoraci, D., Prud’hommeaux, E., & Ptucha, R. (2019). Synthetic Data Augmentation for Improving Low-Resource ASR. WNYISPW 2019.

<a name="variani2014dvectors"></a>Variani, E., Lei, X., McDermott, E., Moreno, I. L., & Gonzalez-Dominguez, J. (2014). Deep neural networks for small footprint text-dependent speaker verification. ICASSP 2014.

<a name="wang2018styletokens"></a>Wang, Y., Stanton, D., Zhang, Y., Skerry-Ryan, R. J., Battenberg, E., Shor, J., Xiao, Y., Ren, F., Jia, Y., & Saurous, R. A. (2018). Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis. PMLR 2018.

<a name="wood2021face"></a>Wood, E., Baltrušaitis, T., Hewitt, C., Dziadzio, S., Johnson, M., Estellers, V., Cashman, T. J., & Shotton, J. (2021). Fake It Till You Make It: Face analysis in the wild using synthetic data alone. CVF 2021.

<a name="yu2020lowresourceoverview"></a>Yu, C., Kang, M., Chen, Y., Wu, J., & Zhao, X. (2020). Acoustic Modeling Based on Deep Learning for Low-Resource Speech Recognition: An Overview. IEEE Access 2020

<a name="zen2007hts"></a>Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko, T., Black, A. W., & Tokuda, K. (2007). The HMM-based Speech Synthesis System (HTS) Version 2.0. SSW6 2007.