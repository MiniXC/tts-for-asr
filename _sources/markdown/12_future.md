# Future Work

## Speaker Diversity

One of the weaker aspects of my FastSpeech 2 TTS system is its ability to capture the characteristics of a diverse set of speakers. Although the SNR variance I introduced improves on this, the diversity of the model is still far behind the diversity found in real data. While this can be observed qualitatively, I set up a way in which this can be quantified: We measure how much a given ASR model improves as we add more and more speakers -- as you can see in {numref}`speakerwer`, adding more speakers to the real data improves WER by orders of magnitude more than synthesizing a larger number of speakers.

```{figure} ../figures/speaker_div.png
---
figclass: boxed
name: speakerwer
height: 250px
---
Effects of different numbers of speakers on real data,
as well as synthetic data with and without SNR.
```

I would like quantify how well different methods from previous work improve on this, for example SCL (speaker consistency loss) [(Wang et al., 2020)](references.html#wang2020scl).

## Synthetic-to-Real Refinement

The mel-spectrogram oversmoothing I explained in the [synthetic speech chapter](02_tts) is a reason the synthesized data is not as diverse as it could be, making it less suitable for ASR. Thankfully a concept from the wider space of synthetic data for deep learning could be used to improve on this **Synthetic-to-Real Refinement** [(Nikolenko, 2021, p. 235)](references.html#nikolenko2021synthetic). This involves training a generative model to convert inputs from the synthetic distribution to outputs in the real distribution. The idea here is to let FastSpeech 2 do most of the heavy lifting using its fast, discriminative training regime, and then let a generative model such as a GAN-, Diffusion or Flow-based model generate more diversity using the FastSpeech 2 output.

## Real-to-Synthetic Refinement

Synthetic speech generated by FastSpeech 2 is fairly regular and well-suited for ASR. For example, a model trained on one hour of real speech in my experiments achieved a WER of 14 when evaluated and trained on real speech, while a model with the same setup using synthetic speech for training **and evaluation** lead to a WER almost 4 times lower (3.8). Since any paired real speech can be resynthesized, we can look at our TTS model from a different angle: as a normalizer. Since ASR models work so well on synthetic speech, the idea of **Real-to-Synthetic Refinement** is to train an external model to convert regular speech to synthetic speech. Since this is close to a many-to-one than a one-to-many problem, this should be an easier task and could lead to better results. This has been successfully done in the computer vision domain by [Zhang et al. (2019)](references.html#zhang2019vr)