# Future Work

## Speaker Diversity

One of the weaker aspects of my FastSpeech 2 TTS system is its ability to capture the characteristics of a diverse set of speakers. Although the SNR variance I introduced improves on this, the diversity of the model is still far behind the diversity found in real data. While this can be observed qualitatively, I set up a way in which this can be quantified: We measure how much a given ASR model improves as we add more and more speakers -- as you can see in {numref}`speakerwer`, adding more speakers to the real data improves WER by orders of magnitude more than synthesizing a larger number of speakers.

```{figure} ../figures/speaker_div.png
---
figclass: boxed
name: speakerwer
height: 250px
---
Effects of different numbers of speakers on real data,
as well as synthetic data with and without SNR.
```

I would like quantify how well different methods from previous work improve on this, for example SCL (speaker consistency loss) [(Wang et al., 2020)](references.html#wang2020scl).

## Synthetic-to-Real Refinement

The mel-spectrogram oversmoothing I explained in the [synthetic speech chapter](02_tts) is a reason the synthesized data is not as diverse as it could be, making it less suitable for ASR. Thankfully a concept from the wider space of synthetic data for deep learning could be used to improve on this **Synthetic-to-Real Refinement** [(Nikolenko, 2021, p. 235)](references.html#nikolenko2021synthetic). This involves training a generative model to convert inputs from the synthetic distribution to outputs in the real distribution. The idea here is to let FastSpeech 2 do most of the heavy lifting using its fast, discriminative training regime, and then let a generative model such as a GAN-, Diffusion or Flow-based model generate more diversity using the FastSpeech 2 output.

## Real-to-Synthetic Refinement

Synthetic speech generated by FastSpeech 2 is fairly regular and well-suited for ASR. For example, a model trained on one hour of real speech in my experiments achieved a WER of 14 when evaluated and trained on real speech, while a model with the same setup using synthetic speech for training **and evaluation** lead to a WER almost 4 times lower (3.8). Since any paired real speech can be resynthesized, we can look at our TTS model from a different angle: as a normalizer. Since ASR models work so well on synthetic speech, the idea of **Real-to-Synthetic Refinement** is to train an external model to convert regular speech to synthetic speech. Since this is close to a many-to-one than a one-to-many problem, this should be an easier task and could lead to better results. This has been successfully done in the computer vision domain by [Zhang et al. (2019)](references.html#zhang2019vr)

## Tokenized Cross-Lingual TTS

Inspired by the recent work using SRs (speech representations) derived from large pre-trained models [(Wu et al., 2022)](references.html#wu2022srvc)), as well as the developments in multilingual G2P (grapheme-to-phoneme) models [(Zhu et al., 2022)](references.html#zhu2022byt5g2p) I would like to explore building the first TTS architecture utilizing both. The main problem is that there is a mismatch between phones generated by G2P models and the SRs which can be extracted without using any labels. Since previous work found both to be closely related however, it could be possible to map outputs of the pre-trained G2P model which are closely related to a given SR token. Tokens without any discovered relationship could be assigned to new outputs the model could learn during training. The FastSpeech 2 model I have used in my work so far could be used to produce synthetic speech from the SR tokens. I'm hoping that encoder in this architecture could be strong due to the benefit of pre-training on the G2P task. Meanwhile, the decoder in this architecture could be pre-trained on unpaired speech data, since the SRs can be extracted without the need for transcriptions.

```{figure} ../figures/srs.svg
---
figclass: boxed
name: srs
---
Proposed architecture for tokenized cross-lingual TTS.
```