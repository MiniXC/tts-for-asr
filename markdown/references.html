
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta property="og:title" content="References" />
  
<meta property="og:type" content="website" />
  
<meta property="og:url" content="cdminix.me/markdown/references.html" />
  
<meta property="og:description" content="AlumÃ¤e, T., & Kong, J. (2021). Combining Hybrid and End-to-End Approaches for the OpenASR20 Challenge. Interspeech 2021. Azadi, S., Olsson, C., Darrell, T., Goodfellow, I., & Odena, A. (2018). Disc..." />
  
<meta property="og:image" content="https://goodresearch.dev/_images/unicorn.png" />
  
<meta property="og:image:alt" content="References" />
  
    <title>References &#8212; TTS for ASR</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
  <!-- add `style` or `link` tags with your CSS `@font-face` declarations here -->
  <!-- ... and optionally preload the `woff2` for snappier page loads -->
  <link rel="preload" href="../_static/et-book/et-book-bold-line-figures/et-book-bold-line-figures.woff" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="../_static/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.woff" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="../_static/et-book/et-book-roman-line-figures/et-book-roman-line-figures.woff" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="../_static/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="../_static/et-book/et-book-semi-bold-old-style-figures/et-book-semi-bold-old-style-figures.woff" as="font" type="font/woff" crossorigin>

    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.59e7d1499aa759519747cb2a1a335dc4.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tufte.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.90c857b2bf8f3d46aa488b0ed8bf60a6.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://cdminix.me/tts-for-asr/markdown/references.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Future Work" href="12_future.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@patrickmineault" />
    <meta name="twitter:title" content="The Good Research Code Handbook" />
    <meta name="twitter:description" content="This handbook is for grad students, postdocs and PIs who do a lot of programming as part of their research. It will teach you, in a practical manner, how to organize your code so that it is easy to understand and works reliably." />
    <meta name="twitter:image" content="https://goodresearch.dev/_images/unicorn.png" />

    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-3Q6LDVNS0X"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){ dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-3Q6LDVNS0X');
    </script>
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/one.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">TTS for ASR</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   TTS for Low-Resource ASR
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Fundmentals
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_other_fields.html">
   Synthetic Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_tts.html">
   Synthetic Speech
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_low_resource_asr.html">
   Low-Resource ASR
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_low_resource_tts.html">
   Low-Resource TTS
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Combining ASR &amp; TTS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="05_augmentation.html">
   Data Augmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_transfer.html">
   Knowledge Transfer
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  My Work
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="08_fastspeech2.html">
   FastSpeech 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_conditioning.html">
   Conditioning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_snr.html">
   SNR Variance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_german-to-english.html">
   German-to-English Transfer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_future.html">
   Future Work
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

<nav class="bd-links" id="nav-social">
    <div class="bd-toc-item">
        <p class="caption">
            <span class="caption-text">Social</span>
        </p>
        <ul class="nav bd-sidenav" style="display:block">
            <li class="toctree-l1"><a class="github-button" href="https://github.com/patrickmineault/codebook" data-icon="octicon-star" data-show-count="true" data-size="large" aria-label="Star patrickmineault/codebook on GitHub">Star on Github</a></li>
            <li class="toctree-l1"><a href="http://eepurl.com/hHgNOH">Subscribe for updates</a></li>
            <li class="toctree-l1"><a href="http://twitter.com/share?text=The+Good+Research+Code+Handbook.+Learn+to+write+readable%2C+maintainable+research+code+in+Python.&url=https://goodresearch.dev&user&via=patrickmineault" class="twitter-share-button" data-text="The Good Research Code Handbook. Learn to write readable, maintainable code in Python." data-via="patrickmineault" data-show-count="false">Tweet this handbook</a></li>
        </ul>
    </div>
</nav>
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/markdown/references.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/cdminix/tts-for-asr"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/cdminix/tts-for-asr/issues/new?title=Issue%20on%20page%20%2Fmarkdown/references.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/cdminix/tts-for-asr/edit/main/./markdown/references.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">Â¶</a></h1>
<p><a name="alumaekong2021hybrid"></a>AlumÃ¤e, T., &amp; Kong, J. (2021). Combining Hybrid and End-to-End Approaches for the OpenASR20 Challenge. Interspeech 2021.</p>
<p><a name="azadi2018rejection"></a>Azadi, S., Olsson, C., Darrell, T., Goodfellow, I., &amp; Odena, A. (2018). Discriminator Rejection Sampling. ICLR 2018.</p>
<p><a name="baevski2019vqwav2vec"></a>Baevski, A., Schneider, S., &amp; Auli, M. (2020). vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations. ICLR 2019.</p>
<p><a name="baevski2020wav2vec2"></a>Baevski, A., Zhou, H., Mohamed, A., &amp; Auli, M. (2020). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. NeurIPS 2020</p>
<p><a name="casanova2022singlespeaker"></a>Casanova, E., Shulby, C., Korolev, A., Junior, A. C., Soares, A. da S., AluÃ­sio, S., &amp; Ponti, M. A. (2022). A single speaker is almost all you need for automatic speech recognition. <a class="reference external" href="http://arxiv.org/abs/2204.00618">arXiv:2204.00618</a>.</p>
<p><a name="chang2022styleeq"></a>Chang, J.-H. R., Shrivastava, A., Koppula, H. S., Zhang, X., &amp; Tuzel, O. (2022). Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models. PMLR 2022.</p>
<p><a name="chen2020cuts"></a>Chen, Z., Rosenberg, A., Zhang, Y., Wang, G., Ramabhadran, B., &amp; Moreno, P. J. (2020). Improving Speech Recognition Using GAN-Based Speech Synthesis and Contrastive Unspoken Text Selection. Interspeech 2020.</p>
<p><a name="chen2021mixmatch"></a>Chen, Z., Rosenberg, A., Zhang, Y., Zen, H., Ghodsi, M., Huang, Y., Emond, J., Wang, G., Ramabhadran, B., &amp; Moreno, P. J. (2021). Semi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation. Interspeech 2021.</p>
<p><a name="chung2019semisuptts"></a>Chung, Y.-A., Wang, Y., Hsu, W.-N., Zhang, Y., &amp; Skerry-Ryan, R. J. (2019). Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis. ICASSP 2019.</p>
<p><a name="conneau2020xlsr53"></a>Conneau, A., Baevski, A., Collobert, R., Mohamed, A., &amp; Auli, M. (2020). Unsupervised Cross-lingual Representation Learning for Speech Recognition. <a class="reference external" href="http://arxiv.org/abs/2006.13979">arXiv:2006.13979</a>.</p>
<p><a name="cooper2020zeroshot"></a>Cooper, E., Lai, C.-I., Yasuda, Y., Fang, F., Wang, X., Chen, N., &amp; Yamagishi, J. (2020). Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings. ICASSP 2020.</p>
<p><a name="dekorte2020encoder"></a>de Korte, M., Kim, J., &amp; Klabbers, E. (2020). Efficient neural speech synthesis for low-resource languages through multilingual modeling. Interspeech 2020.</p>
<p><a name="duyu2020sc"></a>Du, C., &amp; Yu, K. (2020). Speaker Augmentation for Low Resource Speech Recognition. ICASSP 2020.</p>
<p><a name="fang2019pretrained"></a>Fang, W., Chung, Y.-A., &amp; Glass, J. (2019). Towards Transfer Learning for End-to-End Speech Synthesis from Deep Pre-Trained Language Models <a class="reference external" href="https://arxiv.org/abs/1906.07307">arXiv:1906.07307</a>.</p>
<p><a name="fogel2020scrabblegan"></a>Fogel, S., Averbuch-Elor, H., Cohen, S., Mazor, S., &amp; Litman, R. (2020). ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation. CVPR 2020.</p>
<p><a name="gales2009svm"></a>Gales, M. J. F., Ragni, A., AlDamarki, H., &amp; Gautier, C. (2009). Support vector machines for noise robust ASR. ASRU 2009.</p>
<p><a name="graves2006ctc"></a>Graves, A., Fernandez, S., Gomez, F., &amp; Schmidhuber, J. (2006). Connectionist Temporal Classiï¬cation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. ICML 2006.</p>
<p><a name="grffinlim1984"></a>Griffin, D. &amp; Jae Lim. (1984). Signal estimation from modified short-time Fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing 1984.</p>
<p><a name="guo2021conformer"></a>Guo, P., Boyer, F., Chang, X., Hayashi, T., Higuchi, Y., Inaguma, H., Kamo, N., Li, C., Garcia-Romero, D., Shi, J., Shi, J., Watanabe, S., Wei, K., Zhang, W., &amp; Zhang, Y. (2021). Recent Developments on Espnet Toolkit Boosted By Conformer. ICASSP 2021.</p>
<p><a name="habets2010rir"></a>Habets, E. A. P. (2010). Room Impulse Response Generator. Technische Universiteit Eindhoven, Tech. Rep, 2006</p>
<p><a name="hadian2018lfmmi"></a>Hadian, H., Sameti, H., Povey, D., &amp; Khudanpur, S. (2018). End-to-end Speech Recognition Using Lattice-free MMI. Interspeech 2018.</p>
<p><a name="hayashi2018dth"></a>Hayashi, T., Watanabe, S., Zhang, Y., Toda, T., Hori, T., Astudillo, R., &amp; Takeda, K. (2018). Back-Translation-Style Data Augmentation for end-to-end ASR. SLT 2018.</p>
<p><a name="he2021byte"></a>He, M., Yang, J., He, L., &amp; Soong, F. K. (2021). Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis. <a class="reference external" href="http://arxiv.org/abs/2103.03541">arXiv:2103.03541</a></p>
<p><a name="hsu2018vae"></a>Hsu, W.-N., Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Wang, Y., Cao, Y., Jia, Y., Chen, Z., Shen, J., Nguyen, P., &amp; Pang, R. (2018). Hierarchical Generative Modeling for Controllable Speech Synthesis. ICML 2018.</p>
<p><a name="hsu2019noise"></a>Hsu, W.-N., Zhang, Y., Weiss, R. J., Chung, Y.-A., Wang, Y., Wu, Y., &amp; Glass, J. (2019). Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization. ICASSP 2019.</p>
<p><a name="hsu2021hubert"></a>Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., &amp; Mohamed, A. (2021). HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. TASLP 2021.</p>
<p><a name="hu2022synt"></a>Hu, T.-Y., Armandpour, M., Shrivastava, A., Chang, J.-H. R., Koppula, H., &amp; Tuzel, O. (2022). SYNT++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition. ICASSP 2022.</p>
<p><a name="huang2020adapt"></a>Huang, Y., He, L., Wei, W., Gale, W., Li, J., &amp; Gong, Y. (2020). Using Personalized Speech Synthesis and Neural Language Generator for Rapid Speaker Adaptation. ICASSP 2020.</p>
<p><a name="huang2022dth"></a>Huang, W.-P., Chen, P.-C., Huang, S.-F., &amp; Lee, H. (2022). Few-Shot Cross-Lingual TTS Using Transferable Phoneme Embedding. <a class="reference external" href="http://arxiv.org/abs/2206.15427">arXiv:2206.15427</a>.</p>
<p><a name="huybrechts2021vc"></a>Huybrechts, G., Merritt, T., Comini, G., Perz, B., Shah, R., &amp; Lorenzo-Trueba, J. (2021). Low-resource expressive text-to-speech using data augmentation. ICASSP 2021.</p>
<p><a name="jaitly2013vtlp"></a>Jaitly, N., &amp; Hinton, G. E. (2013). Vocal Tract Length Perturbation (VTLP) improves speech recognition. ICML 2013.</p>
<p><a name="jia2018dvecs"></a>Jia, Y., Zhang, Y., Weiss, R., Wang, Q., Shen, J., &amp; Ren, F. (2018). Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis. NeurIPS 2018.</p>
<p><a name="jia2021pngbert"></a>Jia, Y., Zen, H., Shen, J., Zhang, Y., &amp; Wu, Y. (2021). PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS. Interspeech 2021.</p>
<p><a name="kim2021vits"></a>Kim, J., Kong, J., &amp; Son, J. (2021). Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech. PMLR 2021</p>
<p><a name="kingmawelling2014vae"></a>Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes <a class="reference external" href="http://arxiv.org/abs/1312.6114">arXiv:1312.6114</a>.</p>
<p><a name="kuznichov2019leaf"></a>Kuznichov, D., Zvirin, A., Honen, Y., &amp; Kimmel, R. (2019). Data Augmentation for Leaf Segmentation and Counting Tasks in Rosette Plants. CVF 2019.</p>
<p><a name="laptev2020moredata"></a>Laptev, A., Korostik, R., Svischev, A., Andrusenko, A., Medennikov, I., &amp; Rybin, S. (2020). You Do Not Need More Data: Improving End-To-End Speech Recognition by Text-To-Speech Data Augmentation. CISP-BMEI 2020.</p>
<p><a name="li2018ttsasr"></a>Li, J., Gadde, R., Ginsburg, B., &amp; Lavrukhin, V. (2018). Training Neural Speech Recognition Systems with Synthetic Speech Augmentation. <a class="reference external" href="http://arxiv.org/abs/1811.00707">arXiv:1811.00707</a>.</p>
<p><a name="li2019transformertts"></a>Li, N., Liu, S., Liu, Y., Zhao, S., Liu, M., &amp; Zhou, M. (2019). Neural Speech Synthesis with Transformer Network. AAAI 2019.</p>
<p><a name="lim2021w2v2tts"></a>Lim, Y., Kim, N., Yun, S., Kim, S., &amp; Lee, S.-I. (2021). A Preliminary Study on Wav2Vec 2.0 Embeddings for Text-to-Speech. ICTC 2021.</p>
<p><a name="mcauliffe2017mfa"></a>McAuliffe, M., Socolof, M., Mihuc, S., Wagner, M., &amp; Sonderegger, M. (2017). Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi. Interspeech 2017.</p>
<p><a name="nikolenko2021synthetic"></a>Nikolenko, S. I. (2021). Synthetic Data for Deep Learning (Vol. 174). Springer International Publishing.</p>
<p><a name="oord2016wavenet"></a>Oord, A. van den, Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., &amp; Kavukcuoglu, K. (2016). WaveNet: A Generative Model for Raw Audio. <a class="reference external" href="https://arxiv.org/abs/1609.03499">arXiv:1609.03499</a>.</p>
<p><a name="oord2017vqvae"></a>Oord, A. van den, Vinyals, O., &amp; Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. NeurIPS 2017.</p>
<p><a name="park2019specaugment"></a>Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., &amp; Le, Q. V. (2019). SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. Interspeech 2019.</p>
<p><a name="perero2022hybrid"></a>Perero-Codosero, J. M., Espinoza-Cuadros, F. M., &amp; HernÃ¡ndez-GÃ³mez, L. A. (2022). A Comparison of Hybrid and End-to-End ASR Systems for the IberSpeech-RTVE 2020 Speech-to-Text Transcription Challenge. Applied Sciences (2022).</p>
<p><a name="pine2022lowresourcefastspeech"></a>Pine, A., Wells, D., Brinklow, N., Littell, P., &amp; Richmond, K. (2022). Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization. ACL 2022.</p>
<p><a name="povey2006fmllr"></a>Povey, D., &amp; Saon, G. (2006). Feature and model space speaker adaptation with full covariance Gaussians. Interspeech 2006.</p>
<p><a name="prajwaljawahar2021tts"></a>Prajwal, K. R., &amp; Jawahar, C. V. (2021). Data-Efficient Training Strategies for Neural TTS Systems. CODS COMAD 2021.</p>
<p><a name="ren2021fastspeech2"></a>Ren, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., &amp; Liu, T.-Y. (2021). FastSpeech 2: Fast and High-Quality End-to-End Text to Speech. ICLR 2021.</p>
<p><a name="rosenberg2019ttsasr"></a>Rosenberg, A., Zhang, Y., Ramabhadran, B., Jia, Y., Moreno, P., Wu, Y., &amp; Wu, Z. (2019). Speech Recognition with Augmented Synthesized Speech. ASRU 2019.</p>
<p><a name="rossenbach2020ttsasr"></a>Rossenbach, N., Zeyer, A., SchlÃ¼ter, R., &amp; Ney, H. (2020). Generating Synthetic Audio Data for Attention-Based Speech Recognition Systems. ICASSP 2020.</p>
<p><a name="rossenbach2021hybrid"></a>Rossenbach, N., Zeineldeen, M., Hilmes, B., SchlÃ¼ter, R., &amp; Ney, H. (2021). Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures (arXiv:2104.05379). ASRU 2021.</p>
<p><a name="rygaard2015lowresource"></a>Rygaard, L. V. (2015). Using synthesized speech to improve speech recognition for lowresource languages.</p>
<p><a name="shen2018tacotron2"></a>Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerry-Ryan, R. J., Saurous, R. A., Agiomyrgiannakis, Y., &amp; Wu, Y. (2018). Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. ICASSP 2018.</p>
<p><a name="stanton2022speakergen"></a>Stanton, D., Shannon, M., Mariooryad, S., Skerry-Ryan, R., Battenberg, E., Bagby, T., &amp; Kao, D. (2022). Speaker Generation. ICASSP 2022.</p>
<p><a name="stone2020articulatory"></a>Stone, S., Azgin, A., MÃ¤nz, S., &amp; Birkholz, P. (2020). Prospects of articulatory text-to-speech synthesis. ISSP 2020.</p>
<p><a name="sun2020vae"></a>Sun, G., Zhang, Y., Weiss, R. J., Cao, Y., Zen, H., Rosenberg, A., Ramabhadran, B., &amp; Wu, Y. (2020). Generating Diverse and Natural Text-to-Speech Samples Using a Quantized Fine-Grained VAE and Autoregressive Prosody Prior. ICASSP 2020.</p>
<p><a name="tan2021survey"></a>Tan, X., Qin, T., Soong, F., &amp; Liu, T.-Y. (2021). A Survey on Neural Speech Synthesis. <a class="reference external" href="https://arxiv.org/abs/2106.15561">arXiv:2106.15561</a>.</p>
<p><a name="thai2019improvinglowresource"></a>Thai, B., Jimerson, R., Arcoraci, D., Prudâhommeaux, E., &amp; Ptucha, R. (2019). Synthetic Data Augmentation for Improving Low-Resource ASR. WNYISPW 2019.</p>
<p><a name="tjandra2017speechchain"></a>Tjandra, A., Sakti, S., &amp; Nakamura, S. (2017). Listening while Speaking: Speech Chain by Deep Learning. ASRU 2017.</p>
<p><a name="ueno2021dth"></a>Ueno, S., Mimura, M., Sakai, S., &amp; Kawahara, T. (2021). Data Augmentation for ASR Using TTS Via a Discrete Representation. ASRU 2021.</p>
<p><a name="variani2014dvectors"></a>Variani, E., Lei, X., McDermott, E., Moreno, I. L., &amp; Gonzalez-Dominguez, J. (2014). Deep neural networks for small footprint text-dependent speaker verification. ICASSP 2014.</p>
<p><a name="vaswani2017attention"></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., &amp; Polosukhin, I. (2017). Attention is All you Need. NeurIPS (2017).</p>
<p><a name="waibel1989tdnn"></a>Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., &amp; Lang, K. J. (1989). Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing (1989).</p>
<p><a name="wang2015wordvec"></a>Wang, P., Qian, Y., Soong, F. K., He, L., &amp; Zhao, H. (2015). Word embedding for recurrent neural network based TTS synthesis. ICASSP 2015.</p>
<p><a name="wang2018styletokens"></a>Wang, Y., Stanton, D., Zhang, Y., Skerry-Ryan, R. J., Battenberg, E., Shor, J., Xiao, Y., Ren, F., Jia, Y., &amp; Saurous, R. A. (2018). Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis. PMLR 2018.</p>
<p><a name="wang2020scl"></a>Wang, G., Rosenberg, A., Chen, Z., Zhang, Y., Ramabhadran, B., Wu, Y., &amp; Moreno, P. (2020). Improving Speech Recognition Using Consistent Predictions on Synthesized Speech. ICASSP 2020.</p>
<p><a name="wood2021face"></a>Wood, E., BaltruÅ¡aitis, T., Hewitt, C., Dziadzio, S., Johnson, M., Estellers, V., Cashman, T. J., &amp; Shotton, J. (2021). Fake It Till You Make It: Face analysis in the wild using synthetic data alone. CVF 2021.</p>
<p><a name="wu2022srvc"></a>Wu, J., Polyak, A., Taigman, Y., Fong, J., Agrawal, P., &amp; He, Q. (2022). Multilingual Text-To-Speech Training Using Cross Language Voice Conversion And Self-Supervised Learning Of Speech Representations. ICASSP 2022.</p>
<p><a name="yang2020universal"></a>Yang, J., &amp; He, L. (2020). Towards Universal Text-to-Speech. Interspeech 2020.</p>
<p><a name="yang2022multiling"></a>Yang, J., &amp; He, L. (2022). Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training. <a class="reference external" href="http://arxiv.org/abs/2201.08124">arXiv:2201.08124</a>.</p>
<p><a name="yu2020lowresourceoverview"></a>Yu, C., Kang, M., Chen, Y., Wu, J., &amp; Zhao, X. (2020). Acoustic Modeling Based on Deep Learning for Low-Resource Speech Recognition: An Overview. IEEE Access 2020</p>
<p><a name="zen2007hts"></a>Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko, T., Black, A. W., &amp; Tokuda, K. (2007). The HMM-based Speech Synthesis System (HTS) Version 2.0. SSW6 2007.</p>
<p><a name="zhu2022byt5g2p"></a>Zhu, J., Zhang, C., &amp; Jurgens, D. (2022). ByT5 model for massively multilingual grapheme-to-phoneme conversion <a class="reference external" href="http://arxiv.org/abs/2204.03067">arXiv:2204.03067</a>.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "cdminix/tts-for-asr",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="12_future.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Future Work</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By <a href='https://cdminix.me'>Christoph Minixhofer</a> <a href='https://twitter.com/cdminix'><img height='24' width='24' src='_images/twitter.svg' alt='Twitter' style='width:24px'></a><br/>
        
            &copy; Copyright 2022.<br/>
          <div class="extra_footer">
            Licensed under CC-BY 4.0, Template based on <a href='https://goodresearch.dev'>goodresearch.dev</a>.
          </div>
      </p>
    </div>
  </footer>
  <!-- Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>