
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta property="og:title" content="TTS" />
  
<meta property="og:type" content="website" />
  
<meta property="og:url" content="cdminix.me/markdown/tts.html" />
  
<meta property="og:description" content="As I discussed previously, TTS is a way to conditionally generate synthetic speech, with the main condition being text. Here I will give an overview of the different model architectures used for TT..." />
  
<meta property="og:image" content="https://goodresearch.dev/_images/unicorn.png" />
  
<meta property="og:image:alt" content="TTS" />
  
    <title>TTS &#8212; TTS for ASR</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
  <!-- add `style` or `link` tags with your CSS `@font-face` declarations here -->
  <!-- ... and optionally preload the `woff2` for snappier page loads -->
  <link rel="preload" href="../_static/et-book/et-book-bold-line-figures/et-book-bold-line-figures.woff" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="../_static/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.woff" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="../_static/et-book/et-book-roman-line-figures/et-book-roman-line-figures.woff" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="../_static/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="../_static/et-book/et-book-semi-bold-old-style-figures/et-book-semi-bold-old-style-figures.woff" as="font" type="font/woff" crossorigin>

    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.59e7d1499aa759519747cb2a1a335dc4.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tufte.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.90c857b2bf8f3d46aa488b0ed8bf60a6.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://cdminix.me/tts-for-asr/markdown/tts.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Cross-Speaker Transfer" href="cross_speaker.html" />
    <link rel="prev" title="Low-Resource ASR" href="low_resource_asr.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@patrickmineault" />
    <meta name="twitter:title" content="The Good Research Code Handbook" />
    <meta name="twitter:description" content="This handbook is for grad students, postdocs and PIs who do a lot of programming as part of their research. It will teach you, in a practical manner, how to organize your code so that it is easy to understand and works reliably." />
    <meta name="twitter:image" content="https://goodresearch.dev/_images/unicorn.png" />

    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-3Q6LDVNS0X"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){ dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-3Q6LDVNS0X');
    </script>
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/one.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">TTS for ASR</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   TTS for Low-Resource ASR
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Fundmentals
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="other_fields.html">
   Synthetic Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="low_resource_asr.html">
   Low-Resource ASR
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   TTS
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  TTS for Low-Resource
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="cross_speaker.html">
   Cross-Speaker Transfer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cross_lingual.html">
   Cross-Lingual Transfer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="speechchain.html">
   Speech Chain / Backtransformation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Combining ASR &amp; TTS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="augmentation.html">
   Data Augmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="transfer.html">
   Knowledge Transfer
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  My Work
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="fastspeech2.html">
   FastSpeech 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conditioning.html">
   Conditioning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="snr.html">
   SNR Variance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="german-to-english.html">
   German-to-English Transfer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="future.html">
   Future Work
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

<nav class="bd-links" id="nav-social">
    <div class="bd-toc-item">
        <p class="caption">
            <span class="caption-text">Social</span>
        </p>
        <ul class="nav bd-sidenav" style="display:block">
            <li class="toctree-l1"><a class="github-button" href="https://github.com/patrickmineault/codebook" data-icon="octicon-star" data-show-count="true" data-size="large" aria-label="Star patrickmineault/codebook on GitHub">Star on Github</a></li>
            <li class="toctree-l1"><a href="http://eepurl.com/hHgNOH">Subscribe for updates</a></li>
            <li class="toctree-l1"><a href="http://twitter.com/share?text=The+Good+Research+Code+Handbook.+Learn+to+write+readable%2C+maintainable+research+code+in+Python.&url=https://goodresearch.dev&user&via=patrickmineault" class="twitter-share-button" data-text="The Good Research Code Handbook. Learn to write readable, maintainable code in Python." data-via="patrickmineault" data-show-count="false">Tweet this handbook</a></li>
        </ul>
    </div>
</nav>
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/markdown/tts.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/cdminix/tts-for-asr"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/cdminix/tts-for-asr/issues/new?title=Issue%20on%20page%20%2Fmarkdown/tts.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/cdminix/tts-for-asr/edit/main/./markdown/tts.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#autoregressive-models">
   Autoregressive Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-autoregressive-models">
   Non-Autoregressive Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#architecture-adaptations">
   Architecture Adaptations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#controllability">
     Controllability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-speakers">
     Multiple Speakers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#duration-modeling">
     Duration Modeling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-languages">
     Multiple Languages
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#differing-target-representations">
     Differing Target Representations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampling-techniques">
     Sampling Techniques
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="tts">
<h1>TTS<a class="headerlink" href="#tts" title="Permalink to this headline">¶</a></h1>
<p>As I discussed previously, TTS is a way to <em>conditionally generate</em> synthetic speech, with the main condition being text. Here I will give an overview of the different model architectures used for TTS, and how they have been used for <strong>low-resource ASR</strong> specifically. As in many other machine learning fields, deep neural networks are currently the best-performing systems for TTS. They can be roughly divided into <strong>autoregressive</strong> and <strong>non-autoregressive</strong> models. <strong>Autoregressive</strong> models require the previous output at inference time, while <strong>Non-autoregressive</strong> models do not. The advantages of <strong>non-autoregressive</strong> models are faster inference time and a reduced training-inference mismatch (as autoregressive models effectively use teacher-forcing) <a class="reference external" href="references.html#tan2021survey">(Tan et al., 2021)</a>.
I do not give a complete overview of TTS here, but mostly focus on architectures that have been used for <strong>TTS-for-ASR</strong>.</p>
<div class="section" id="autoregressive-models">
<h2>Autoregressive Models<a class="headerlink" href="#autoregressive-models" title="Permalink to this headline">¶</a></h2>
<p>WaveNet <a class="reference external" href="references.html#oord2016wavenet">(Oord et al., 2016)</a> is “regarded as the first modern neural TTS model” <a class="reference external" href="references.html#tan2021survey">(Tan et al., 2021)</a> and is an autoregressive CNN which directly predicts waveforms. The next big leap in synthesis quality came with Tacotron 2 <a class="reference external" href="references.html#shen2018tacotron2">(Shen et al., 2018)</a>, which first predicts Mel-spectrograms using an encoder-decoder RNN, and predicts waveforms using the Mel-spectrograms as inputs to an additional WaveNet, rather than using an algorithmic approach like the Griffin-Lim algorithm <a class="reference external" href="references.html#grffinlim1984">(Griffin &amp; Lim, 1984)</a>. TransformerTTS <a class="reference external" href="references.html#li2019transformertts">(Li et al., 2019)</a> was the first work to bring the transformer encoder-decoder architecture very popular in NLP to TTS. While many recent works focus on non-autoregressive speech synthesis, Style Equalisation <a class="reference external" href="references.html#chang2022styleeq">(Chang et al., 2022)</a>, which uses LSTM encoder and decoder networks, is a notable exception.</p>
</div>
<div class="section" id="non-autoregressive-models">
<h2>Non-Autoregressive Models<a class="headerlink" href="#non-autoregressive-models" title="Permalink to this headline">¶</a></h2>
<p>While non-autoregressive models have become quite popular for TTS, they haven’t been used for <strong>TTS-for-ASR</strong> extensively. As far as I know (as of August 2022), the VITS <a class="reference external" href="references.html#kim2021vits">(Kim et al., 2021)</a> architecture, which generates speech using a variational autoencoder with normalising flows and adversarial training, has been used. One previous work (Ueno et al., 2021) uses FastSpeech 2. In my own work, I have been using FastSpeech 2 <a class="reference external" href="references.html#ren2021fastspeech2">(Ren et al., 2021)</a> as well, since it has been successfully used for extremely low-resource cases, producing comparable quality with 1 hour of data compared to Tacotron 2 with 10 hours of data <a class="reference external" href="references.html#pine2022lowresourcefastspeech">(Pine et al., 2022)</a>. I explain this architecture in detail in the <a class="reference internal" href="fastspeech2.html"><span class="doc std std-doc">FastSpeech 2 chapter</span></a>.</p>
<!-- 
SCL - speaker consistency loss
VAE - variational autoencoder
GST - global style tokens
SDP - stochastic duration predictor
LE - language embeddings
ASC - adversarial speaker classification
DTM - direct-to-mel
SA - speaker adaptation
DTH - direct-to-hidden
SC - speaker classification

-->
</div>
<div class="section" id="architecture-adaptations">
<h2>Architecture Adaptations<a class="headerlink" href="#architecture-adaptations" title="Permalink to this headline">¶</a></h2>
<p>The model architectures I have summarised above have been modified to better account for different factors suitable for <strong>TTS-for-ASR</strong>.</p>
<div class="boxed figure align-default" id="id2">
<img alt="../_images/tts-table.svg" src="../_images/tts-table.svg" /><p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">The different base TTS models and their modifications for <strong>TTS-for-ASR</strong>.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="controllability">
<h3>Controllability<a class="headerlink" href="#controllability" title="Permalink to this headline">¶</a></h3>
<p>Controllable (also called expressive) TTS aims at better solving the one-to-many problem in TTS, where one utterance can be realized in many different ways. For example, if I simply trained a model using L1-loss, the produced mel-spectrograms will likely be oversmoothed, as the model learns to predict the average rather than the different possible realizations of each text input. This is done by explicitly or implicitly modeling <strong>variation information</strong> <a class="reference external" href="references.html#tan2021survey">(Tan et al., 2021)</a>. The <strong>variation information</strong> used falls into four categories:</p>
<ul class="simple">
<li><p><em>Text Content</em>, in the form of characters or phonemes is the most basic variation information. Some works use representations from pre-trained language models to enhance this information (for example <a class="reference external" href="references.html#fang2019pretrained">Fang et al., 2019</a>).</p></li>
<li><p><em>Speaker information</em> obviously informs the model which speaker is to be synthesized. Sometimes this is done in a naïve way by simply giving each speaker an ID, often speaker representations such as d-vectors <a class="reference external" href="references.html#variani2014dvectors">(Variani et al., 2014)</a> are derived from an external model. I explain the different techniques used for this the <a class="reference external" href="#multiple-speakers">next section</a>.</p></li>
<li><p><em>Prosody, style and emotion</em> are seen as the key for controllable TTS <a class="reference external" href="references.html#tan2021survey">(Tan et al., 2021)</a>. This includes, pitch, duration, energy, etc.</p></li>
<li><p><em>Environmental information</em> captures factors about the recording environment, such as reverberation or the amount and type of background noise or microphone characteristics. Work in this area often focuses on disentangling speaker information from their environment, which isn’t easy since both are usually correlated <a class="reference external" href="references.html#hsu2019noise">(Hsu et al., 2019)</a></p></li>
</ul>
<p>Controllability can be achieved <em>implicitly</em> by using methods which learn a latent space (which is ideally independent of the text content) to model the style of the speech. The methods that have been used for <strong>TTS-for-ASR</strong> are shown in the “Controllability” column in Fig. 4, and are:</p>
<ul class="simple">
<li><p><em>GST</em> (Global Style Tokens) by <a class="reference external" href="references.html#wang2018styletokens">Wang et al. (2018)</a> use an autoencoder trained with reconstruction loss which learns to represent reference speech samples as a set of 10 quantized style tokens, which can be used for inference to control the style of the output. For <strong>TTS-for-ASR</strong> this technique has been used by <a class="reference external" href="references.html#li2018ttsasr">Li et al. (2018)</a> and <a class="reference external" href="references.html#rossenbach2020ttsasr">Rossenbach et al. (2020)</a>.</p></li>
<li><p><em>VAE</em> (Variational Autoencoder) <a class="reference external" href="references.html#kingmawelling2014vae">(Kingma &amp; Welling, 2014)</a> are the most common solution for controllability in <strong>TTS-for-ASR</strong> <a class="footnote-reference brackets" href="#vae" id="id1">1</a>. Opposed to a regular autoencoder, VAEs are trained using ELBO (Evidence lower bound), which regularizes the latent space by encoding reference samples as a distribution (which is later sampled from), rather than a single point. VQ-VAE <a class="reference external" href="references.html#oord2017vqvae">(Oord et al., 2017)</a> extends VAEs by quantizing the latent space using codebook vectors.</p></li>
<li><p><em>SE</em> (Style Equalization) <a class="reference external" href="references.html#chang2022styleeq">(Chang et al., 2022)</a> is a new technique which learns a style transformation function which transforms a latent representation of a reference sample into the style of the target training sample. The model learns to disentangle style and content and at inference time, the style transformation is simply not performed.</p></li>
</ul>
<p><em>Explicit</em> controllability can be achieved by computing certain statistics of the data and making this information available to the training process. For example, in FastSpeech 2 <a class="reference external" href="references.html#ren2021fastspeech2">(Ren et al., 2021)</a>, pitch, energy and duration are extracted from the original data, and subsequently predicted by the model and added to the hidden sequence. Since these predicted values are called “Variances” in FastSpeech 2, I abbreviate this approach as “VAR” in Fig. 4. While this can lead to faster convergence and is robust to low-resource data conditions and is more interpretable than a “black box” latent space, there is a fundamental mismatch between training and inference variances when using this approach. I believe this approach is well-suited for low-resource <strong>TTS-for-ASR</strong>, and use it in my own work, which you can read more about in the <a class="reference internal" href="conditioning.html"><span class="doc std std-doc">conditioning chapter</span></a>.</p>
<!-- TODO: add figure showing the difference between explicit and implicit controllable TTS -->
</div>
<div class="section" id="multiple-speakers">
<h3>Multiple Speakers<a class="headerlink" href="#multiple-speakers" title="Permalink to this headline">¶</a></h3>
<p>While most multi-speaker TTS models used for <strong>TTS-for-ASR</strong> make use of d-vectors <a class="reference external" href="references.html#variani2014dvectors">(Variani et al., 2014)</a>, they do so in different ways. Wang et al. (2020) and Casanova et al. (2022) use SCL (speaker consistency loss), backpropagates the loss of the model used to generate the d-vectors through the TTS model without updating the d-vector model weights. Du &amp; Yu (2020) add a separate SC (speaker classifier), while Huang et al. (2020) focus SA (speaker adaptation) with just a few minutes of audio. Chen et al. (2021), meanwhile, focus on disentangling the speaker representation from the language to allow for cross-lingual transfer using ASC (adversarial speaker classification). I discuss these methods in more detail in the <a class="reference internal" href="cross_speaker.html"><span class="doc std std-doc">Cross-Speaker Transfer</span></a> and <a class="reference internal" href="cross_lingual.html"><span class="doc std std-doc">Cross-Lingual Transfer</span></a> chapters.</p>
<!-- TODO: add citations -->
</div>
<div class="section" id="duration-modeling">
<h3>Duration Modeling<a class="headerlink" href="#duration-modeling" title="Permalink to this headline">¶</a></h3>
<p>In modern TTS systems, duration is either modeled by <em>learned attention alignments</em> or explicitly predicted.
Attention alignments often struggle with word skipping/repeating and/or attention collapse <a class="reference external" href="references.html#tan2021survey">(Tan et al., 2021)</a>, but have the benefit that duration does not have to be extracted from the data in a separate step. When durations are explicitly predicted, they have to be extracted first, either by using a teacher model utilizing learned attention alignments or using forced alignment (McAuliffe et al., 2017). Since the same one-to-many oversmoothing problem that exists for mel-spectrograms applies to durations as well, a SDP (stochastic duration predictor) using normalizing flows has been used by Casanova et al. (2022).</p>
<!-- TODO: add citations -->
</div>
<div class="section" id="multiple-languages">
<h3>Multiple Languages<a class="headerlink" href="#multiple-languages" title="Permalink to this headline">¶</a></h3>
<p>Multi-lingual TTS systems are only useful for <strong>TTS-for-ASR</strong> approaches when knowledge from one ore more high resource languages can be transferred to low-resource languages. To this end, both Chen et al. (2021) and Casanova et al. (2022) us LE (language embeddings), where the former relies on disentangling language and speaker information using adversarial classification, while the latter relies on SCL (speaker consistency loss).</p>
</div>
<div class="section" id="differing-target-representations">
<h3>Differing Target Representations<a class="headerlink" href="#differing-target-representations" title="Permalink to this headline">¶</a></h3>
<p>Another way to adapt TTS architectures for ASR is to predict a target representation other than raw audio. As long as this representation can be derived from raw audio, ASR models can be trained on it. Chen et al. (2021) do this by doing DTM (direct-to-mel) prediction, predicting mel-spectrograms without using a vocoder. Hayashi et al. (2018) and Ueno et al. (2021) predict use DTH (direct-to-hidden) to predict the hidden representations of an ASR model, where the former use their own model and the latter use discrete IDs derived from vq-wav2vec (Baevski et al., 2019). The advantage of this is that discrete IDs are easier to predict than continuos values, reducing the mismatch between synthetic and real data.</p>
</div>
<div class="section" id="sampling-techniques">
<h3>Sampling Techniques<a class="headerlink" href="#sampling-techniques" title="Permalink to this headline">¶</a></h3>
<p>Since it’s essentially free to generate more data once a model is trained, data can be overgenerated and a subset more suitable for <strong>TTS-to-ASR</strong> can be sampled. Hu et al. (2022) use RS (rejection sampling), where they use the score of an external discriminator to reject samples least likely to be classified as real speech. In my own work, I artificially increase diversity (ID in Fig. 4) by sampling with more extreme variation information than in the original data. I explain this in detail in the <a class="reference internal" href="conditioning.html"><span class="doc std std-doc">conditioning chapter</span></a>.</p>
<!-- TODO: add speaker oversampling (artificial speakers) -->
<!-- TODO: add citations --><hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="vae"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>As you can see in Fig. 4, they are used by Rosenberg et al. (2019), Laptev et al. (2020), Sun et al. (2020), Du &amp; Yu (2020), Chen al. (2021) and Casanova et al. (2022).</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "cdminix/tts-for-asr",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="low_resource_asr.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Low-Resource ASR</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="cross_speaker.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Cross-Speaker Transfer</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By <a href='https://cdminix.me'>Christoph Minixhofer</a> <a href='https://twitter.com/cdminix'><img height='24' width='24' src='_images/twitter.svg' alt='Twitter' style='width:24px'></a><br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            Licensed under CC-BY 4.0, Template based on <a href='https://goodresearch.dev'>goodresearch.dev</a>.
          </div>
      </p>
    </div>
  </footer>
  <!-- Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>