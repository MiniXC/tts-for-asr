# References

<a name="alumaekong2021hybrid"></a>Alumäe, T., & Kong, J. (2021). Combining Hybrid and End-to-End Approaches for the OpenASR20 Challenge. Interspeech 2021.

<a name="azadi2018rejection"></a>Azadi, S., Olsson, C., Darrell, T., Goodfellow, I., & Odena, A. (2018). Discriminator Rejection Sampling. ICLR 2018.

<a name="baevski2019vqwav2vec"></a>Baevski, A., Schneider, S., & Auli, M. (2020). vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations. ICLR 2019.

<a name="baevski2020wav2vec2"></a>Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. NeurIPS 2020

<a name="casanova2022singlespeaker"></a>Casanova, E., Shulby, C., Korolev, A., Junior, A. C., Soares, A. da S., Aluísio, S., & Ponti, M. A. (2022). A single speaker is almost all you need for automatic speech recognition. [arXiv:2204.00618](http://arxiv.org/abs/2204.00618).

<a name="chang2022styleeq"></a>Chang, J.-H. R., Shrivastava, A., Koppula, H. S., Zhang, X., & Tuzel, O. (2022). Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models. PMLR 2022.

<a name="chen2020cuts"></a>Chen, Z., Rosenberg, A., Zhang, Y., Wang, G., Ramabhadran, B., & Moreno, P. J. (2020). Improving Speech Recognition Using GAN-Based Speech Synthesis and Contrastive Unspoken Text Selection. Interspeech 2020.

<a name="chen2021mixmatch"></a>Chen, Z., Rosenberg, A., Zhang, Y., Zen, H., Ghodsi, M., Huang, Y., Emond, J., Wang, G., Ramabhadran, B., & Moreno, P. J. (2021). Semi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation. Interspeech 2021.

<a name="chien2021fastspeech2dvec"></a>Chien, C.-M., Lin, J.-H., Huang, C., Hsu, P., & Lee, H. (2021). Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech. ICASSP 2021.

<a name="chung2019semisuptts"></a>Chung, Y.-A., Wang, Y., Hsu, W.-N., Zhang, Y., & Skerry-Ryan, R. J. (2019). Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis. ICASSP 2019.

<a name="conneau2020xlsr53"></a>Conneau, A., Baevski, A., Collobert, R., Mohamed, A., & Auli, M. (2020). Unsupervised Cross-lingual Representation Learning for Speech Recognition. [arXiv:2006.13979](http://arxiv.org/abs/2006.13979).

<a name="cooper2020zeroshot"></a>Cooper, E., Lai, C.-I., Yasuda, Y., Fang, F., Wang, X., Chen, N., & Yamagishi, J. (2020). Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings. ICASSP 2020.

<a name="dekorte2020encoder"></a>de Korte, M., Kim, J., & Klabbers, E. (2020). Efficient neural speech synthesis for low-resource languages through multilingual modeling. Interspeech 2020. 

<a name="duyu2020sc"></a>Du, C., & Yu, K. (2020). Speaker Augmentation for Low Resource Speech Recognition. ICASSP 2020.

<a name="fang2019pretrained"></a>Fang, W., Chung, Y.-A., & Glass, J. (2019). Towards Transfer Learning for End-to-End Speech Synthesis from Deep Pre-Trained Language Models [arXiv:1906.07307](https://arxiv.org/abs/1906.07307).

<a name="fazel2021medical"></a>Fazel, A., Yang, W., Liu, Y., Barra-Chicote, R., Meng, Y., Maas, R., & Droppo, J. (2021). SynthASR: Unlocking Synthetic Data for Speech Recognition. Interspeech 2021.

<a name="fogel2020scrabblegan"></a>Fogel, S., Averbuch-Elor, H., Cohen, S., Mazor, S., & Litman, R. (2020). ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation. CVPR 2020.

<a name="fuglede2004jsd"></a>Fuglede, B., & Topsoe, F. (2004). Jensen-Shannon divergence and Hilbert space embedding. International Symposium OnInformation Theory, 2004. ISIT 2004.

<a name="gales2009svm"></a>Gales, M. J. F., Ragni, A., AlDamarki, H., & Gautier, C. (2009). Support vector machines for noise robust ASR. ASRU 2009.

<a name="gulati2020conformer"></a>Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., & Pang, R. (2020). Conformer: Convolution-augmented Transformer for Speech Recognition. [arXiv:2005.08100](http://arxiv.org/abs/2005.08100).

<a name="graves2006ctc"></a>Graves, A., Fernandez, S., Gomez, F., & Schmidhuber, J. (2006). Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. ICML 2006.

<a name="grffinlim1984"></a>Griffin, D. & Jae Lim. (1984). Signal estimation from modified short-time Fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing 1984.

<a name="guo2021conformer"></a>Guo, P., Boyer, F., Chang, X., Hayashi, T., Higuchi, Y., Inaguma, H., Kamo, N., Li, C., Garcia-Romero, D., Shi, J., Shi, J., Watanabe, S., Wei, K., Zhang, W., & Zhang, Y. (2021). Recent Developments on Espnet Toolkit Boosted By Conformer. ICASSP 2021.

<a name="habets2010rir"></a>Habets, E. A. P. (2010). Room Impulse Response Generator. Technische Universiteit Eindhoven, Tech. Rep, 2006

<a name="hadian2018lfmmi"></a>Hadian, H., Sameti, H., Povey, D., & Khudanpur, S. (2018). End-to-end Speech Recognition Using Lattice-free MMI. Interspeech 2018.

<a name="hayashi2018dth"></a>Hayashi, T., Watanabe, S., Zhang, Y., Toda, T., Hori, T., Astudillo, R., & Takeda, K. (2018). Back-Translation-Style Data Augmentation for end-to-end ASR. SLT 2018.

<a name="he2021byte"></a>He, M., Yang, J., He, L., & Soong, F. K. (2021). Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis. [arXiv:2103.03541](http://arxiv.org/abs/2103.03541)

<a name="hsu2018vae"></a>Hsu, W.-N., Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Wang, Y., Cao, Y., Jia, Y., Chen, Z., Shen, J., Nguyen, P., & Pang, R. (2018). Hierarchical Generative Modeling for Controllable Speech Synthesis. ICML 2018.

<a name="hsu2019noise"></a>Hsu, W.-N., Zhang, Y., Weiss, R. J., Chung, Y.-A., Wang, Y., Wu, Y., & Glass, J. (2019). Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization. ICASSP 2019.

<a name="hsu2021hubert"></a>Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., & Mohamed, A. (2021). HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. TASLP 2021.

<a name="hu2022synt"></a>Hu, T.-Y., Armandpour, M., Shrivastava, A., Chang, J.-H. R., Koppula, H., & Tuzel, O. (2022). SYNT++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition. ICASSP 2022.

<a name="huang2020adapt"></a>Huang, Y., He, L., Wei, W., Gale, W., Li, J., & Gong, Y. (2020). Using Personalized Speech Synthesis and Neural Language Generator for Rapid Speaker Adaptation. ICASSP 2020.

<a name="huang2022dth"></a>Huang, W.-P., Chen, P.-C., Huang, S.-F., & Lee, H. (2022). Few-Shot Cross-Lingual TTS Using Transferable Phoneme Embedding. [arXiv:2206.15427](http://arxiv.org/abs/2206.15427).

<a name="huybrechts2021vc"></a>Huybrechts, G., Merritt, T., Comini, G., Perz, B., Shah, R., & Lorenzo-Trueba, J. (2021). Low-resource expressive text-to-speech using data augmentation. ICASSP 2021.

<a name="jaitly2013vtlp"></a>Jaitly, N., & Hinton, G. E. (2013). Vocal Tract Length Perturbation (VTLP) improves speech recognition. ICML 2013.

<a name="jia2018dvecs"></a>Jia, Y., Zhang, Y., Weiss, R., Wang, Q., Shen, J., & Ren, F. (2018). Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis. NeurIPS 2018.

<a name="jia2021pngbert"></a>Jia, Y., Zen, H., Shen, J., Zhang, Y., & Wu, Y. (2021). PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS. Interspeech 2021.

<a name="kimstern2008wada"></a>Kim, C., & Stern, R. M. (2008). Robust Signal-to-Noise Ratio Estimation Based on Waveform Amplitude Distribution Analysis. ISCA 2008.

<a name="kim2021vits"></a>Kim, J., Kong, J., & Son, J. (2021). Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech. PMLR 2021

<a name="kingmawelling2014vae"></a>Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes [arXiv:1312.6114](http://arxiv.org/abs/1312.6114).

<a name="kong2020hifigan"></a>Kong, J., Kim, J., & Bae, J. (2020). HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis. NeurIPS 2020.

<a name="kuznichov2019leaf"></a>Kuznichov, D., Zvirin, A., Honen, Y., & Kimmel, R. (2019). Data Augmentation for Leaf Segmentation and Counting Tasks in Rosette Plants. CVF 2019.

<a name="laptev2020moredata"></a>Laptev, A., Korostik, R., Svischev, A., Andrusenko, A., Medennikov, I., & Rybin, S. (2020). You Do Not Need More Data: Improving End-To-End Speech Recognition by Text-To-Speech Data Augmentation. CISP-BMEI 2020.

<a name="li2018ttsasr"></a>Li, J., Gadde, R., Ginsburg, B., & Lavrukhin, V. (2018). Training Neural Speech Recognition Systems with Synthetic Speech Augmentation. [arXiv:1811.00707](http://arxiv.org/abs/1811.00707).

<a name="li2019transformertts"></a>Li, N., Liu, S., Liu, Y., Zhao, S., Liu, M., & Zhou, M. (2019). Neural Speech Synthesis with Transformer Network. AAAI 2019.

<a name="lim2021w2v2tts"></a>Lim, Y., Kim, N., Yun, S., Kim, S., & Lee, S.-I. (2021). A Preliminary Study on Wav2Vec 2.0 Embeddings for Text-to-Speech. ICTC 2021.

<a name="luo2021lightspeech"></a>Luo, R., Tan, X., Wang, R., Qin, T., Li, J., Zhao, S., Chen, E., & Liu, T.-Y. (2021). LightSpeech: Lightweight and Fast Text to Speech with Neural Architecture Search. ICASSP 2021.

<a name="mcauliffe2017mfa"></a>McAuliffe, M., Socolof, M., Mihuc, S., Wagner, M., & Sonderegger, M. (2017). Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi. Interspeech 2017.

<a name="moran2019phoible"></a>Moran, Steven & McCloy, Daniel (eds.) 2019. PHOIBLE 2.0. Jena: Max Planck Institute for the Science of Human History. (Available online at http://phoible.org, Accessed on 2022-08-22.) 

<a name="nagrani2017voxceleb"></a>Nagrani, A., Chung, J. S., & Zisserman, A. (2017). VoxCeleb: A large-scale speaker identification dataset. Interspeech 2017.

<a name="nikolenko2021synthetic"></a>Nikolenko, S. I. (2021). Synthetic Data for Deep Learning (Vol. 174). Springer International Publishing.

<a name="oord2016wavenet"></a>Oord, A. van den, Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., & Kavukcuoglu, K. (2016). WaveNet: A Generative Model for Raw Audio. [arXiv:1609.03499](https://arxiv.org/abs/1609.03499).

<a name="oord2017vqvae"></a>Oord, A. van den, Vinyals, O., & Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. NeurIPS 2017.

<a name="panayotov2015librispeech"></a>Panayotov, V., Chen, G., Povey, D., & Khudanpur, S. (2015). Librispeech: An ASR corpus based on public domain audio books. ICASSP 2015.

<a name="park2019specaugment"></a>Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., & Le, Q. V. (2019). SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. Interspeech 2019.

<a name="perero2022hybrid"></a>Perero-Codosero, J. M., Espinoza-Cuadros, F. M., & Hernández-Gómez, L. A. (2022). A Comparison of Hybrid and End-to-End ASR Systems for the IberSpeech-RTVE 2020 Speech-to-Text Transcription Challenge. Applied Sciences (2022).

<a name="pine2022lowresourcefastspeech"></a>Pine, A., Wells, D., Brinklow, N., Littell, P., & Richmond, K. (2022). Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization. ACL 2022.

<a name="povey2006fmllr"></a>Povey, D., & Saon, G. (2006). Feature and model space speaker adaptation with full covariance Gaussians. Interspeech 2006.

<a name="prajwaljawahar2021tts"></a>Prajwal, K. R., & Jawahar, C. V. (2021). Data-Efficient Training Strategies for Neural TTS Systems. CODS COMAD 2021.

<a name="ren2021fastspeech2"></a>Ren, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., & Liu, T.-Y. (2021). FastSpeech 2: Fast and High-Quality End-to-End Text to Speech. ICLR 2021.

<a name="rosenberg2019ttsasr"></a>Rosenberg, A., Zhang, Y., Ramabhadran, B., Jia, Y., Moreno, P., Wu, Y., & Wu, Z. (2019). Speech Recognition with Augmented Synthesized Speech. ASRU 2019.

<a name="rossenbach2020ttsasr"></a>Rossenbach, N., Zeyer, A., Schlüter, R., & Ney, H. (2020). Generating Synthetic Audio Data for Attention-Based Speech Recognition Systems. ICASSP 2020.

<a name="rossenbach2021hybrid"></a>Rossenbach, N., Zeineldeen, M., Hilmes, B., Schlüter, R., & Ney, H. (2021). Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. ASRU 2021.

<a name="rygaard2015lowresource"></a>Rygaard, L. V. (2015). Using synthesized speech to improve speech recognition for lowresource languages.

<a name="schultz2002globalphone"></a>Schultz, T. (2002). Globalphone: A multilingual speech and text database developed at karlsruhe university. ICSLP 2002.

<a name="shen2018tacotron2"></a>Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerry-Ryan, R. J., Saurous, R. A., Agiomyrgiannakis, Y., & Wu, Y. (2018). Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. ICASSP 2018.

<a name="sifre2014depthwise"></a>Sifre, L., & Mallat, S. (2014). Rigid-Motion Scattering for Texture Classification. [arXiv:1403.1687](http://arxiv.org/abs/1403.1687).

<a name="stanton2022speakergen"></a>Stanton, D., Shannon, M., Mariooryad, S., Skerry-Ryan, R., Battenberg, E., Bagby, T., & Kao, D. (2022). Speaker Generation. ICASSP 2022.

<a name="stone2020articulatory"></a>Stone, S., Azgin, A., Mänz, S., & Birkholz, P. (2020). Prospects of articulatory text-to-speech synthesis. ISSP 2020.

<a name="sun2020vae"></a>Sun, G., Zhang, Y., Weiss, R. J., Cao, Y., Zen, H., Rosenberg, A., Ramabhadran, B., & Wu, Y. (2020). Generating Diverse and Natural Text-to-Speech Samples Using a Quantized Fine-Grained VAE and Autoregressive Prosody Prior. ICASSP 2020.

<a name="tan2021survey"></a>Tan, X., Qin, T., Soong, F., & Liu, T.-Y. (2021). A Survey on Neural Speech Synthesis. [arXiv:2106.15561](https://arxiv.org/abs/2106.15561).

<a name="thai2019improvinglowresource"></a>Thai, B., Jimerson, R., Arcoraci, D., Prud’hommeaux, E., & Ptucha, R. (2019). Synthetic Data Augmentation for Improving Low-Resource ASR. WNYISPW 2019.

<a name="tjandra2017speechchain"></a>Tjandra, A., Sakti, S., & Nakamura, S. (2017). Listening while Speaking: Speech Chain by Deep Learning. ASRU 2017.

<a name="ueno2021dth"></a>Ueno, S., Mimura, M., Sakai, S., & Kawahara, T. (2021). Data Augmentation for ASR Using TTS Via a Discrete Representation. ASRU 2021.

<a name="variani2014dvectors"></a>Variani, E., Lei, X., McDermott, E., Moreno, I. L., & Gonzalez-Dominguez, J. (2014). Deep neural networks for small footprint text-dependent speaker verification. ICASSP 2014.

<a name="vaswani2017attention"></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is All you Need. NeurIPS (2017).

<a name="viikkilaurila1998cmvn"></a>Viikki, O., & Laurila, K. (1998). Cepstral domain segmental feature vector normalization for noise robust speech recognition. Speech Communication (1998).

<a name="waibel1989tdnn"></a>Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. J. (1989). Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing (1989).

<a name="wan2018g2e"></a>Wan, L., Wang, Q., Papir, A., & Moreno, I. L. (2020). Generalized End-to-End Loss for Speaker Verification. ICASSP 2018.

<a name="wang2015wordvec"></a>Wang, P., Qian, Y., Soong, F. K., He, L., & Zhao, H. (2015). Word embedding for recurrent neural network based TTS synthesis. ICASSP 2015.

<a name="wang2018styletokens"></a>Wang, Y., Stanton, D., Zhang, Y., Skerry-Ryan, R. J., Battenberg, E., Shor, J., Xiao, Y., Ren, F., Jia, Y., & Saurous, R. A. (2018). Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis. PMLR 2018.

<a name="wang2020scl"></a>Wang, G., Rosenberg, A., Chen, Z., Zhang, Y., Ramabhadran, B., Wu, Y., & Moreno, P. (2020). Improving Speech Recognition Using Consistent Predictions on Synthesized Speech. ICASSP 2020.

<a name="wood2021face"></a>Wood, E., Baltrušaitis, T., Hewitt, C., Dziadzio, S., Johnson, M., Estellers, V., Cashman, T. J., & Shotton, J. (2021). Fake It Till You Make It: Face analysis in the wild using synthetic data alone. CVF 2021.

<a name="wu2022srvc"></a>Wu, J., Polyak, A., Taigman, Y., Fong, J., Agrawal, P., & He, Q. (2022). Multilingual Text-To-Speech Training Using Cross Language Voice Conversion And Self-Supervised Learning Of Speech Representations. ICASSP 2022.

<a name="xin2021scl"></a>Xin, D., Saito, Y., Takamichi, S., Koriyama, T., & Saruwatari, H. (2021). Cross-Lingual Speaker Adaptation Using Domain Adaptation and Speaker Consistency Loss for Text-To-Speech Synthesis. Interspeech 2021.

<a name="xu2020lrspeech"></a>Xu, J., Tan, X., Ren, Y., Qin, T., Li, J., Zhao, S., & Liu, T.-Y. (2020). LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition. ACM SIGKDD 2020.

<a name="yang2020universal"></a>Yang, J., & He, L. (2020). Towards Universal Text-to-Speech. Interspeech 2020.

<a name="yang2022multiling"></a>Yang, J., & He, L. (2022). Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training. [arXiv:2201.08124](http://arxiv.org/abs/2201.08124).

<a name="yu2020lowresourceoverview"></a>Yu, C., Kang, M., Chen, Y., Wu, J., & Zhao, X. (2020). Acoustic Modeling Based on Deep Learning for Low-Resource Speech Recognition: An Overview. IEEE Access 2020

<a name="zen2007hts"></a>Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko, T., Black, A. W., & Tokuda, K. (2007). The HMM-based Speech Synthesis System (HTS) Version 2.0. SSW6 2007.

<a name="zen2019libritts"></a>Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., & Wu, Y. (2019). LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. Interspeech 2019.

<a name="zhu2022byt5g2p"></a>Zhu, J., Zhang, C., & Jurgens, D. (2022). ByT5 model for massively multilingual grapheme-to-phoneme conversion [arXiv:2204.03067](http://arxiv.org/abs/2204.03067).