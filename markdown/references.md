# References

<a name="chang2022styleeq"></a>Chang, J.-H. R., Shrivastava, A., Koppula, H. S., Zhang, X., & Tuzel, O. (2022). Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models (2022). PMLR 2022.

<a name="fogel2020scrabblegan"></a>Fogel, S., Averbuch-Elor, H., Cohen, S., Mazor, S., & Litman, R. (2020). ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation. CVPR 2020.

<a name="gales2009svm"></a>Gales, M. J. F., Ragni, A., AlDamarki, H., & Gautier, C. (2009). Support vector machines for noise robust ASR. ASRU 2009.

<a name="grffinlim1984"></a>Griffin, D. & Jae Lim. (1984). Signal estimation from modified short-time Fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing 1984.

<a name="hu2022synt"></a>Hu, T.-Y., Armandpour, M., Shrivastava, A., Chang, J.-H. R., Koppula, H., & Tuzel, O. (2022). SYNT++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition. ICASSP 2022.

<a name="hsu2018vae"></a>Hsu, W.-N., Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Wang, Y., Cao, Y., Jia, Y., Chen, Z., Shen, J., Nguyen, P., & Pang, R. (2018). Hierarchical Generative Modeling for Controllable Speech Synthesis, ICML 2018.

<a name="jaitly2013vtlp"></a>Jaitly, N., & Hinton, G. E. (n.d.). Vocal Tract Length Perturbation (VTLP) improves speech recognition. ICML 2013.

<a name="kim2021vits"></a>Kim, J., Kong, J., & Son, J. (n.d.). Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech. PMLR 2021

<a name="kuznichov2019leaf"></a>Kuznichov, D., Zvirin, A., Honen, Y., & Kimmel, R. (2019). Data Augmentation for Leaf Segmentation and Counting Tasks in Rosette Plants. CVF 2019.

<a name="li2019transformertts"></a>Li, N., Liu, S., Liu, Y., Zhao, S., Liu, M., & Zhou, M. (2019). Neural Speech Synthesis with Transformer Network (arXiv:1809.08895). AAAI 2019.

<a name="nikolenko2021synthetic"></a>Nikolenko, S. I. (2021). Synthetic Data for Deep Learning (Vol. 174). Springer International Publishing.

<a name="oord2016wavenet"></a>Oord, A. van den, Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., & Kavukcuoglu, K. (2016). WaveNet: A Generative Model for Raw Audio. [arXiv:1609.03499](https://arxiv.org/abs/1609.03499).

<a name="park2019specaugment"></a>Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., & Le, Q. V. (2019). SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. Interspeech 2019.

<a name="pine2022lowresourcefastspeech"></a>Pine, A., Wells, D., Brinklow, N., Littell, P., & Richmond, K. (2022). Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization. ACL 2022.

<a name="povey2006fmllr"></a>Povey, D., & Saon, G. (2006). Feature and model space speaker adaptation with full covariance Gaussians. Interspeech 2006.

<a name="ren2021fastspeech2"></a>Ren, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., & Liu, T.-Y. (2021). FastSpeech 2: Fast and High-Quality End-to-End Text to Speech. ICLR 2021.

<a name="rygaard2015lowresource"></a>Rygaard, L. V. (2015). Using synthesized speech to improve speech recognition for lowresource languages.

<a name="shen2018tacotron2"></a>Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerry-Ryan, R. J., Saurous, R. A., Agiomyrgiannakis, Y., & Wu, Y. (2018). Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions ICASSP 2018.

<a name="stone2020articulatory"></a>Stone, S., Azgin, A., Mänz, S., & Birkholz, P. (2020). Prospects of articulatory text-to-speech synthesis. ISSP 2020.

<a name="tan2021survey"></a>Tan, X., Qin, T., Soong, F., & Liu, T.-Y. (2021). A Survey on Neural Speech Synthesis. [arXiv:2106.15561](https://arxiv.org/abs/2106.15561).

<a name="thai2019improvinglowresource"></a>Thai, B., Jimerson, R., Arcoraci, D., Prud’hommeaux, E., & Ptucha, R. (2019). Synthetic Data Augmentation for Improving Low-Resource ASR. WNYISPW 2019.

<a name="wood2021face"></a>Wood, E., Baltrušaitis, T., Hewitt, C., Dziadzio, S., Johnson, M., Estellers, V., Cashman, T. J., & Shotton, J. (2021). Fake It Till You Make It: Face analysis in the wild using synthetic data alone. CVF 2021.

<a name="yu2020lowresourceoverview"></a>Yu, C., Kang, M., Chen, Y., Wu, J., & Zhao, X. (2020). Acoustic Modeling Based on Deep Learning for Low-Resource Speech Recognition: An Overview. IEEE Access 2020

<a name="zen2007hts"></a>Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko, T., Black, A. W., & Tokuda, K. (2007). The HMM-based Speech Synthesis System (HTS) Version 2.0. SSW6 2007.