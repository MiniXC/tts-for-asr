# Data Augmentation

Almost all **TTS-for-ASR** approaches use data augmentation [^synthonly], since real speech data is augmented with synthetic data.
In this chapter, I focus on how this augmentation can best be achieved, and which factors influence the ASR performance of a model trained with synthetic-augmented speech data.

## Filtering/Sampling

Before any synthetic data is added to the real data, it can be helpful to filter the data or to sample from the model in a way beneficial to ASR. This can be helpful because the amount of data that can be generated using a TTS model is usually not the limiting factor in a **TTS-for-ASR** system. This means we can generate more data than we need and then *filter* it. Or, seeing it from the perspective of *sampling*, we only generate samples fulfilling certain conditions in the first place. One way to filter out low-quality TTS results is computing the WER (word error rate) on said results and dropping ones above a threshold of 20% WER [(Rosenberg et al., 2019)](references.html#rosenberg2019ttsasr). [Wang et al. (2020)](references.html#wang2020scl) performed copy synthesis using scheduled sampling at inference time, feeding the model ground-truth predictions at inference time with a probability of $0.2$, leading to more high-quality inference and lower WER. [Sun et al. (2020)](references.html#sun2020vae) performed copy synthesis as well, with the addition that their VAE-TTS systems allowed them to sample from different latent style representations at inference time to produce different styles of the same utterance. [Casanova et al.(2022)](references.html#casanova2022singlespeaker) randomly sample from their model temperature ($T$), duration predictor temperature ($T_{DP}$) and length multiplier ($L$) to increase diversity. Unfortunately, they do not evaluate the effect of these parameters on ASR results. Another new method to make the TTS data distribution closer to the real data by sampling is discriminator rejection sampling [(Azadi et al., 2018)](references.html#azadi2018rejection). This uses the probability produced by a discriminator to reject samples which are far away from the real speech distribution, and was successfully used by [Hu et al. (2022)](references.html#hu2022synt).

But the TTS model is not the only component that can be sampled from when generating synthetic speech -- [Huang et al. (2020)](references.html#huang2020adapt) sample from an LSTM language model with diversity constraints (e.g. penalizing repeating tokens) to generate the inputs for their TTS system. [Chen et al., (2021)](references.html#chen2021mixmatch) also sample on a lexical level. They do so by filtering a very large text dataset down to 30 million samples per language using contrastive unspoken text selection [(Chen et al., 2020)](references.html#chen2020cuts).

## Combining the Data

Several **TTS-for-ASR** systems simply combine real and synthetic data using a 50:50 split ([Li et al., 2018](references.html#li2018ttsasr); [Rosenberg et al., 2019](references.html#rosenberg2019ttsasr); [Wang et al., 2020](references.html#wang2020scl)) -- the former find that a 33:66 (real:synthetic) split degrades performance compare to a 50:50 split but still slightly outperforms the baseline. [Hayashi et al. (2018)](references.html#hayashi2018dth), who directly generate the hidden states of an ASR model using their method, use a 22:78 split successfully, but only when using acoustic rather than hidden features for the real data to avoid overfitting. Thanks to increased style diversity enabled by a VAE, [Sun et al. (2020)](references.html#sun2020vae) improve their ASR's WER by 16% (relative) with a 9:91 split.

## Augmentation$^2$

Synthetic speech is often combined with other augmentation methods, such as SpecAugment [(Park et al., 2019)](references.html#park2019specaugment) or RIR (room impulse response) generation [(Habets, 2010)](references.html#habets2010rir), to introduce more diversity and to make it harder for ASR systems to overfit to the synthetic data ([Wang et al., 2020](references.html#wang2020scl); [Sun et al. (2020)](references.html#sun2020vae); [Casanova et al., 2022](references.html#casanova2022singlespeaker))

[^synthonly]: Except when training on synthetic data only.